{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if model_name == \"bert\":\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        model = BertModel.from_pretrained(\"bert-base-uncased\").to(device) \n",
    "        return tokenizer, model, device\n",
    "    elif model_name == \"bge-m3\":\n",
    "        model = SentenceTransformer(\"BAAI/bge-large-en\").to(device)  \n",
    "        return None, model, device \n",
    "    elif model_name == \"all-MiniLM-L6-v2\":\n",
    "        model = SentenceTransformer(\"all-MiniLM-L6-v2\").to(device) \n",
    "        return None, model, device \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(data, model_name, output_dir):\n",
    "    tokenizer, model, device = load_model(model_name)\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "\n",
    "    for item in data:\n",
    "        if model_name == \"bert\":\n",
    "            inputs = tokenizer(item[\"caption\"], return_tensors=\"pt\", padding=True, truncation=True).to(device)  # Move inputs to GPU\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                last_hidden_state = outputs.last_hidden_state\n",
    "                embedding = torch.mean(last_hidden_state, dim=1).squeeze().cpu().numpy()  # Move result to CPU\n",
    "        else:\n",
    "            embedding = model.encode(item[\"caption\"], device=device)  \n",
    "\n",
    "        embeddings.append(embedding)\n",
    "        metadata.append({\n",
    "            \"video_id\": item[\"video_id\"],\n",
    "            \"timestamp\": item[\"timestamp\"],\n",
    "            \"frame_image_path\": item[\"frame_image_path\"],\n",
    "            \"caption\": item[\"caption\"]\n",
    "        })\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    np.save(os.path.join(output_dir, f\"{model_name}_embeddings.npy\"), np.array(embeddings))\n",
    "    with open(os.path.join(output_dir, f\"{model_name}_metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embeddings_path, metadata_path):\n",
    "    embeddings = np.load(embeddings_path)\n",
    "    with open(metadata_path, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    return embeddings, metadata\n",
    "\n",
    "\n",
    "def create_and_save_faiss_index(embeddings, output_path):\n",
    "    d = embeddings.shape[1] \n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, output_path)\n",
    "\n",
    "\n",
    "def load_faiss_index(input_path):\n",
    "    return faiss.read_index(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index, metadata, model, model_name, device, top_k=5):\n",
    "    if model_name == \"bert\":\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True).to(device) \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            query_embedding = torch.mean(last_hidden_state, dim=1).squeeze().cpu().numpy() \n",
    "    else:\n",
    "        query_embedding = model.encode(query, device=device) \n",
    "\n",
    "    query_embedding = query_embedding.reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        results.append(metadata[idx])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing with all-MiniLM-L6-v2 ===\n",
      "Video ID: Fz9HnTVx52g, Timestamp: 0:01:52.862\n",
      "Caption: 체크무늬 드레스를 입고 웃고 있는 금발 머리의 여자\n",
      "Frame Path: own_dataset_video/frames/Fz9HnTVx52g_112.863.jpg\n",
      "\n",
      "Video ID: Fz9HnTVx52g, Timestamp: 0:01:53.780\n",
      "Caption: 체크무늬 드레스를 입고 웃고 있는 금발 머리의 여자\n",
      "Frame Path: own_dataset_video/frames/Fz9HnTVx52g_113.780.jpg\n",
      "\n",
      "Video ID: Fz9HnTVx52g, Timestamp: 0:01:53.322\n",
      "Caption: 체크무늬 드레스를 입고 웃고 있는 금발 머리의 여자\n",
      "Frame Path: own_dataset_video/frames/Fz9HnTVx52g_113.322.jpg\n",
      "\n",
      "Video ID: n1lbpj6868o, Timestamp: 0:00:52.761\n",
      "Caption: 체크무늬 양복과 넥타이를 입은 남자가 뭔가를 보고 있다\n",
      "Frame Path: own_dataset_video/frames/n1lbpj6868o_52.761.jpg\n",
      "\n",
      "Video ID: n1lbpj6868o, Timestamp: 0:00:51.843\n",
      "Caption: 체크무늬 양복과 넥타이를 입은 남자가 뭔가를 보고 있다\n",
      "Frame Path: own_dataset_video/frames/n1lbpj6868o_51.843.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    frames_dir = os.path.join(\"frames\")\n",
    "    captions_file = os.path.join(\"frame_captions_0.5sec.json\")\n",
    "    output_dir = os.path.join(\"embeddings\")\n",
    "\n",
    "    with open(captions_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    models = [\"all-MiniLM-L6-v2\"] #[\"bert\", \"bge-m3\", \"all-MiniLM-L6-v2\"]\n",
    "\n",
    "    for model_name in models:\n",
    "        print(f\"\\n=== Processing with {model_name} ===\")\n",
    "\n",
    "        embeddings_path = os.path.join(output_dir, f\"{model_name}_embeddings.npy\")\n",
    "        metadata_path = os.path.join(output_dir, f\"{model_name}_metadata.json\")\n",
    "        index_path = os.path.join(output_dir, f\"{model_name}_faiss.index\")\n",
    "\n",
    "        if not os.path.exists(embeddings_path) or not os.path.exists(metadata_path):\n",
    "            print(f\"Generating embeddings for {model_name}...\")\n",
    "            save_embeddings(data, model_name, output_dir)\n",
    "\n",
    "        if not os.path.exists(index_path):\n",
    "            print(f\"Creating FAISS index for {model_name}...\")\n",
    "            embeddings, _ = load_embeddings(embeddings_path, metadata_path)\n",
    "            create_and_save_faiss_index(embeddings, index_path)\n",
    "\n",
    "        embeddings, metadata = load_embeddings(embeddings_path, metadata_path)\n",
    "        index = load_faiss_index(index_path)\n",
    "\n",
    "        query = \"밤의 설원에서 숲으로 둘러싸인 따뜻한 불빛이 비치는 통나무집\"\n",
    "        tokenizer, model, device = load_model(model_name) \n",
    "        results = search(query, index, metadata, model, model_name, device) \n",
    "\n",
    "        for res in results:\n",
    "            print(f\"Video ID: {res['video_id']}, Timestamp: {res['timestamp']}\")\n",
    "            print(f\"Caption: {res['caption']}\")\n",
    "            print(f\"Frame Path: {res['frame_image_path']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
